# -*- coding: utf-8 -*-
"""Catboost_Final_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aBMQUEK_HMZKpxP25KLXJuArQ6V0pdwz
"""

!pip install catboost

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.metrics import (roc_auc_score, classification_report,
                             confusion_matrix, mean_squared_error, mean_absolute_error, r2_score)
from catboost import CatBoostClassifier, CatBoostRegressor, Pool
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("üö¶ SEISMIC TRAFFIC LIGHT PREDICTION SYSTEM v2 - IMPROVED")
print("=" * 80)
print("Changes: 3-Class Traffic Light | COVID Downsampling | Fixed Indexing")
print("=" * 80)

# -------------------------
# Load data
# -------------------------
file_path = Path('/content/drive/MyDrive/operational_seismic_linear_decay121.csv')
df = pd.read_csv(file_path, low_memory=False)
print(f"\n‚úì Loaded {len(df):,} records with {df.shape[1]} features")

# ==============================================================================
# STEP 1: DATA CLEANING
# ==============================================================================
print("\n" + "=" * 80)
print("STEP 1: DATA CLEANING")
print("=" * 80)

# Replace -999 sentinel values with 0
sentinel_cols = ['pgv_max', 'magnitude', 'hourly_seismicity_rate']
for col in sentinel_cols:
    if col in df.columns:
        mask = df[col] == -999.0
        print(f"  {col}: Replacing {mask.sum():,} sentinel values (-999) with 0")
        df.loc[mask, col] = 0

# Parse datetime columns
datetime_cols = ['recorded_at', 'phase_started_at', 'phase_production_ended_at',
                 'phase_ended_at', 'occurred_at']
for col in datetime_cols:
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors='coerce')

# Sort chronologically
if 'recorded_at' in df.columns:
    df = df.sort_values('recorded_at').reset_index(drop=True)
    print("  ‚úì Data sorted by recorded_at")
else:
    df = df.reset_index(drop=True)

# ==============================================================================
# STEP 2: MULTI-TARGET CREATION (3-CLASS TRAFFIC LIGHT)
# ==============================================================================
print("\n" + "=" * 80)
print("STEP 2: MULTI-TARGET CREATION")
print("=" * 80)

# Target 1: Binary event occurrence
df['event_occurs'] = ((df['magnitude'] >= 0.17) | (df['hourly_seismicity_rate'] > 0)).astype(int)

# Target 2: Event magnitude (only for events that occur)
df['event_magnitude'] = df['magnitude'].copy()

# Create 3-class traffic light
def classify_traffic_light_3class(magnitude):
    if magnitude >= 1:
        return 2  # üî¥ RED - High/Moderate Risk (merged orange into red)
    elif magnitude >= 0.17:
        return 1  # üü° YELLOW - Low Risk
    else:
        return 0  # üü¢ GREEN - Safe

df['traffic_light'] = df['event_magnitude'].apply(classify_traffic_light_3class)

print(f"\nüìä EVENT OCCURRENCE DISTRIBUTION:")
event_dist = df['event_occurs'].value_counts()
print(f"  No Event (0): {event_dist.get(0, 0):,} ({event_dist.get(0, 0)/len(df)*100:.2f}%)")
print(f"  Event (1):    {event_dist.get(1, 0):,} ({event_dist.get(1, 0)/len(df)*100:.2f}%)")

print(f"\nüö¶ 3-CLASS TRAFFIC LIGHT DISTRIBUTION (Before COVID Downsampling):")
traffic_dist = df['traffic_light'].value_counts().sort_index()
for level, count in traffic_dist.items():
    labels = {0: "üü¢ GREEN (Safe)", 1: "üü° YELLOW (Low)", 2: "üî¥ RED (High/Moderate)"}
    print(f"  {labels.get(level, f'Level {level}')}: {count:,} ({count/len(df)*100:.2f}%)")

print(f"\nüìà MAGNITUDE STATISTICS (for events):")
events_df = df[df['event_occurs'] == 1].copy()
if len(events_df) > 0:
    print(f"  Count: {len(events_df):,}")
    print(f"  Min:   {events_df['event_magnitude'].min():.4f}")
    print(f"  Mean:  {events_df['event_magnitude'].mean():.4f}")
    print(f"  Median: {events_df['event_magnitude'].median():.4f}")
    print(f"  Max:   {events_df['event_magnitude'].max():.4f}")

# ==============================================================================
# STEP 2.5: COVID PERIOD DOWNSAMPLING (2021-2022)
# ==============================================================================
print("\n" + "=" * 80)
print("STEP 2.5: COVID PERIOD GREEN CLASS DOWNSAMPLING")
print("=" * 80)

if 'recorded_at' in df.columns:
    # Identify COVID period (2021-2022)
    covid_mask = (df['recorded_at'].dt.year >= 2021) & (df['recorded_at'].dt.year <= 2022)
    green_mask = df['traffic_light'] == 0
    covid_green_mask = covid_mask & green_mask

    print(f"\n  Total records in COVID period (2021-2022): {covid_mask.sum():,}")
    print(f"  GREEN samples in COVID period: {covid_green_mask.sum():,}")

    # Keep all non-GREEN and non-COVID samples
    keep_mask = ~covid_green_mask

    # Downsample COVID GREEN samples (keep 10%)
    covid_green_indices = df[covid_green_mask].index.tolist()
    downsample_size = int(len(covid_green_indices) * 0.10)

    np.random.seed(42)
    keep_covid_green_indices = np.random.choice(covid_green_indices,
                                                size=downsample_size,
                                                replace=False)

    # Combine: all non-COVID-GREEN + downsampled COVID-GREEN
    final_keep_indices = df[keep_mask].index.tolist() + list(keep_covid_green_indices)

    # Filter dataframe
    df_original_size = len(df)
    df = df.loc[final_keep_indices].copy()
    df = df.sort_values('recorded_at').reset_index(drop=True)

    print(f"  Kept {downsample_size:,} out of {len(covid_green_indices):,} COVID GREEN samples (10%)")
    print(f"  Dataset size: {df_original_size:,} ‚Üí {len(df):,} ({len(df)/df_original_size*100:.1f}%)")

    # Show updated distribution
    print(f"\nüö¶ 3-CLASS TRAFFIC LIGHT DISTRIBUTION (After COVID Downsampling):")
    traffic_dist_after = df['traffic_light'].value_counts().sort_index()
    for level, count in traffic_dist_after.items():
        labels = {0: "üü¢ GREEN (Safe)", 1: "üü° YELLOW (Low)", 2: "üî¥ RED (High/Moderate)"}
        print(f"  {labels.get(level, f'Level {level}')}: {count:,} ({count/len(df)*100:.2f}%)")
else:
    print("  ‚ö†Ô∏è 'recorded_at' not found - skipping COVID downsampling")

# ==============================================================================
# STEP 3: FEATURE ENGINEERING
# ==============================================================================
print("\n" + "=" * 80)
print("STEP 3: FEATURE ENGINEERING")
print("=" * 80)

# Temporal features
if 'recorded_at' in df.columns:
    df['hour'] = df['recorded_at'].dt.hour
    df['day_of_week'] = df['recorded_at'].dt.dayofweek
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
    df['month'] = df['recorded_at'].dt.month

# Operational phase duration
if 'phase_started_at' in df.columns and 'recorded_at' in df.columns:
    df['phase_duration_hours'] = (df['recorded_at'] - df['phase_started_at']).dt.total_seconds() / 3600

# Rolling statistics
for window in [6, 12, 24, 48]:
    if 'inj_temp' in df.columns:
        df[f'inj_temp_rolling_mean_{window}h'] = df['inj_temp'].rolling(window, min_periods=1).mean()
        df[f'inj_temp_rolling_std_{window}h'] = df['inj_temp'].rolling(window, min_periods=1).std()
    if 'inj_whp' in df.columns:
        df[f'inj_whp_rolling_mean_{window}h'] = df['inj_whp'].rolling(window, min_periods=1).mean()
    if 'prod_flow' in df.columns:
        df[f'prod_flow_rolling_max_{window}h'] = df['prod_flow'].rolling(window, min_periods=1).max()

# Rate of change
for c in ['inj_temp', 'inj_whp', 'cum_inj_energy', 'prod_temp']:
    if c in df.columns:
        df[f'{c}_change'] = df[c].diff()

# Pressure and temperature differences
if 'inj_whp' in df.columns and 'prod_whp' in df.columns:
    df['pressure_diff'] = df['inj_whp'] - df['prod_whp']
if 'inj_temp' in df.columns and 'prod_temp' in df.columns:
    df['temp_diff'] = df['inj_temp'] - df['prod_temp']

# Energy efficiency metrics
if 'inj_energy' in df.columns and 'inj_flow' in df.columns:
    df['inj_energy_per_flow'] = df['inj_energy'] / (df['inj_flow'] + 1e-6)
if 'cooling_energy' in df.columns and 'inj_energy' in df.columns:
    df['cooling_efficiency'] = df['cooling_energy'] / (df['inj_energy'] + 1e-6)

# Cumulative stress indicators
if 'cum_inj_energy' in df.columns and 'cum_volume' in df.columns:
    df['cum_energy_normalized'] = df['cum_inj_energy'] / (df['cum_volume'] + 1e-6)

# Interaction features
if 'inj_temp' in df.columns and 'inj_whp' in df.columns:
    df['temp_pressure_interaction'] = df['inj_temp'] * df['inj_whp']
if 'inj_flow' in df.columns and 'inj_whp' in df.columns:
    df['flow_pressure_interaction'] = df['inj_flow'] * df['inj_whp']

print("  ‚úì Feature engineering complete")

# ==============================================================================
# STEP 4: DATA PREPARATION
# ==============================================================================
print("\n" + "=" * 80)
print("STEP 4: DATA PREPARATION (TIME-BASED SPLIT)")
print("=" * 80)

exclude_cols = [
    'recorded_at', 'phase_started_at', 'phase_production_ended_at',
    'phase_ended_at', 'occurred_at', 'event_occurs', 'event_magnitude',
    'traffic_light', 'hourly_seismicity_rate', 'rounded', 'adjusted', 'magnitude'
]

feature_cols = [c for c in df.columns if c not in exclude_cols]
X_all = df[feature_cols].copy()
y_event = df['event_occurs'].values  # Convert to numpy array
y_magnitude = df['event_magnitude'].values  # Convert to numpy array
y_traffic = df['traffic_light'].values  # Convert to numpy array

print(f"\n  Total features: {X_all.shape[1]}")

# Chronological split
split_idx = int(len(df) * 0.8)
X_train = X_all.iloc[:split_idx].copy()
X_test = X_all.iloc[split_idx:].copy()
y_event_train = y_event[:split_idx]
y_event_test = y_event[split_idx:]
y_magnitude_train = y_magnitude[:split_idx]
y_magnitude_test = y_magnitude[split_idx:]
y_traffic_train = y_traffic[:split_idx]
y_traffic_test = y_traffic[split_idx:]

if 'recorded_at' in df.columns:
    train_times = df['recorded_at'].iloc[:split_idx]
    test_times = df['recorded_at'].iloc[split_idx:]
    print(f"\n  Train: {train_times.min()} -> {train_times.max()}")
    print(f"  Test:  {test_times.min()} -> {test_times.max()}")

print(f"\n  Train: {len(X_train):,} samples ({y_event_train.mean()*100:.4f}% events)")
print(f"  Test:  {len(X_test):,} samples ({y_event_test.mean()*100:.4f}% events)")

# Imputation
numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()

X_train = X_train.replace([np.inf, -np.inf], np.nan)
X_test = X_test.replace([np.inf, -np.inf], np.nan)

train_medians = X_train[numeric_cols].median()
X_train.loc[:, numeric_cols] = X_train[numeric_cols].fillna(train_medians)
X_test.loc[:, numeric_cols] = X_test[numeric_cols].fillna(train_medians)

for col in categorical_cols:
    X_train.loc[:, col] = X_train[col].astype(str).fillna('missing')
    X_test.loc[:, col] = X_test[col].astype(str).fillna('missing')

cat_features = [i for i, col in enumerate(X_train.columns) if col in categorical_cols]

# Calculate class weight
train_pos = (y_event_train == 1).sum()
train_neg = (y_event_train == 0).sum()
scale_pos_weight = train_neg / train_pos if train_pos > 0 else 1

print(f"\n  Scale pos weight: {scale_pos_weight:.2f}")

# ==============================================================================
# STEP 5: MODEL 1 - EVENT OCCURRENCE PREDICTION
# ==============================================================================
print("\n" + "=" * 80)
print("STEP 5: TRAINING MODEL 1 - EVENT OCCURRENCE PREDICTOR")
print("=" * 80)

train_pool_event = Pool(X_train, y_event_train, cat_features=cat_features)
test_pool_event = Pool(X_test, y_event_test, cat_features=cat_features)

model_event = CatBoostClassifier(
    iterations=3000,
    learning_rate=0.02,
    depth=6,
    l2_leaf_reg=10,
    min_data_in_leaf=50,
    max_leaves=20,
    grow_policy='Lossguide',
    scale_pos_weight=scale_pos_weight,
    loss_function='Logloss',
    eval_metric='F1',
    custom_metric=['AUC', 'Recall', 'Precision'],
    early_stopping_rounds=200,
    bootstrap_type='Bernoulli',
    subsample=0.8,
    random_seed=42,
    verbose=200,
    use_best_model=True
)

print("\nüîÑ Training Event Occurrence Model...")
model_event.fit(train_pool_event, eval_set=test_pool_event, use_best_model=True, plot=False)
print(f"‚úì Event Model Complete! Best iteration: {model_event.get_best_iteration()}")

# Save model
model_event.save_model("seismic_event_occurrence_model_v2.cbm")
print("üíæ Saved: seismic_event_occurrence_model_v2.cbm")

# ==============================================================================
# STEP 6: MODEL 2 - MAGNITUDE PREDICTION (IMPROVED INDEXING)
# ==============================================================================
print("\n" + "=" * 80)
print("STEP 6: TRAINING MODEL 2 - MAGNITUDE PREDICTOR (FIXED)")
print("=" * 80)

# Create boolean masks for events
event_mask_train = y_event_train == 1
event_mask_test = y_event_test == 1

print(f"\n  Events in train: {event_mask_train.sum():,} out of {len(event_mask_train):,}")
print(f"  Events in test:  {event_mask_test.sum():,} out of {len(event_mask_test):,}")

# Filter data using iloc to avoid indexing issues
X_train_events = X_train.iloc[event_mask_train].copy()
y_magnitude_train_events = y_magnitude_train[event_mask_train].copy()

X_test_events = X_test.iloc[event_mask_test].copy()
y_magnitude_test_events = y_magnitude_test[event_mask_test].copy()

# Reset indices to avoid warnings
X_train_events = X_train_events.reset_index(drop=True)
X_test_events = X_test_events.reset_index(drop=True)

print(f"\n  Training samples: {len(X_train_events):,}")
print(f"  Testing samples:  {len(X_test_events):,}")

if len(X_train_events) > 10 and len(X_test_events) > 5:
    train_pool_mag = Pool(X_train_events, y_magnitude_train_events, cat_features=cat_features)
    test_pool_mag = Pool(X_test_events, y_magnitude_test_events, cat_features=cat_features)

    model_magnitude = CatBoostRegressor(
        iterations=2500,
        learning_rate=0.02,
        depth=7,
        l2_leaf_reg=5,
        min_data_in_leaf=10,
        max_leaves=28,
        grow_policy='Lossguide',
        loss_function='RMSE',
        eval_metric='MAE',
        custom_metric=['RMSE', 'R2'],
        early_stopping_rounds=200,
        bootstrap_type='Bayesian',
        bagging_temperature=1.0,
        random_seed=42,
        verbose=200,
        use_best_model=True
    )

    print("\nüîÑ Training Magnitude Prediction Model...")
    model_magnitude.fit(train_pool_mag, eval_set=test_pool_mag, use_best_model=True, plot=False)
    print(f"‚úì Magnitude Model Complete! Best iteration: {model_magnitude.get_best_iteration()}")

    # Save model
    model_magnitude.save_model("seismic_magnitude_model_v2.cbm")
    print("üíæ Saved: seismic_magnitude_model_v2.cbm")
else:
    print("‚ö†Ô∏è Not enough event samples for magnitude model!")
    model_magnitude = None

# ==============================================================================
# STEP 7: MODEL 3 - 3-CLASS TRAFFIC LIGHT CLASSIFIER
# ==============================================================================
print("\n" + "=" * 80)
print("STEP 7: TRAINING MODEL 3 - 3-CLASS TRAFFIC LIGHT CLASSIFIER")
print("=" * 80)

train_pool_traffic = Pool(X_train, y_traffic_train, cat_features=cat_features)
test_pool_traffic = Pool(X_test, y_traffic_test, cat_features=cat_features)

# Calculate class weights for 3-class
traffic_train_series = pd.Series(y_traffic_train)
traffic_counts = traffic_train_series.value_counts()
total_samples = len(y_traffic_train)
class_weights = []
for cls in sorted(traffic_counts.index):
    weight = total_samples / (len(traffic_counts) * traffic_counts[cls])
    class_weights.append(weight)

print(f"\n  3-Class weights: {[f'{w:.2f}' for w in class_weights]}")

model_traffic = CatBoostClassifier(
    iterations=3000,
    learning_rate=0.02,
    depth=6,
    l2_leaf_reg=10,
    min_data_in_leaf=40,
    max_leaves=20,
    grow_policy='Lossguide',
    loss_function='MultiClass',
    eval_metric='TotalF1',
    classes_count=3,  # GREEN, YELLOW, RED (removed ORANGE)
    early_stopping_rounds=200,
    bootstrap_type='Bernoulli',
    subsample=0.8,
    class_weights=class_weights,
    random_seed=42,
    verbose=200,
    use_best_model=True
)

print("\nüîÑ Training 3-Class Traffic Light Model...")
model_traffic.fit(train_pool_traffic, eval_set=test_pool_traffic, use_best_model=True, plot=False)
print(f"‚úì Traffic Light Model Complete! Best iteration: {model_traffic.get_best_iteration()}")

# Save model
model_traffic.save_model("seismic_traffic_light_3class_model_v2.cbm")
print("üíæ Saved: seismic_traffic_light_3class_model_v2.cbm")

# Save training medians
with open('train_medians_v2.pkl', 'wb') as f:
    pickle.dump(train_medians, f)
print("üíæ Saved: train_medians_v2.pkl")

# ==============================================================================
# STEP 8: COMPREHENSIVE EVALUATION
# ==============================================================================
print("\n" + "=" * 80)
print("STEP 8: MODEL EVALUATION")
print("=" * 80)

# Model 1: Event Occurrence
print("\nüìä MODEL 1: EVENT OCCURRENCE PREDICTION")
print("-" * 80)
y_event_proba = model_event.predict_proba(X_test)[:, 1]

# Find optimal threshold
from sklearn.metrics import precision_recall_curve
precisions, recalls, thresholds = precision_recall_curve(y_event_test, y_event_proba)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
optimal_idx = np.argmax(f1_scores[:-1])
optimal_threshold = thresholds[optimal_idx]

y_event_pred = (y_event_proba >= optimal_threshold).astype(int)

print(f"Optimal Threshold: {optimal_threshold:.6f}")
print(f"AUC: {roc_auc_score(y_event_test, y_event_proba):.6f}")
print("\nClassification Report:")
print(classification_report(y_event_test, y_event_pred,
                            target_names=['No Event', 'Event'], zero_division=0))

cm_event = confusion_matrix(y_event_test, y_event_pred)
tn, fp, fn, tp = cm_event.ravel()
print(f"\nConfusion Matrix:")
print(f"  TN: {tn:,} | FP: {fp:,}")
print(f"  FN: {fn:,} | TP: {tp:,}")

# Save threshold
with open('optimal_event_threshold_v2.txt', 'w') as f:
    f.write(f"{optimal_threshold}\n")

# Model 2: Magnitude Prediction
if model_magnitude is not None and len(X_test_events) > 0:
    print("\nüìä MODEL 2: MAGNITUDE PREDICTION (FOR EVENTS)")
    print("-" * 80)
    y_magnitude_pred = model_magnitude.predict(X_test_events)

    rmse = np.sqrt(mean_squared_error(y_magnitude_test_events, y_magnitude_pred))
    mae = mean_absolute_error(y_magnitude_test_events, y_magnitude_pred)
    r2 = r2_score(y_magnitude_test_events, y_magnitude_pred)

    print(f"RMSE: {rmse:.4f}")
    print(f"MAE:  {mae:.4f}")
    print(f"R¬≤:   {r2:.4f}")

    print(f"\nMagnitude Statistics:")
    print(f"  Actual   - Mean: {y_magnitude_test_events.mean():.4f}, Std: {y_magnitude_test_events.std():.4f}, Range: [{y_magnitude_test_events.min():.4f}, {y_magnitude_test_events.max():.4f}]")
    print(f"  Predicted - Mean: {y_magnitude_pred.mean():.4f}, Std: {y_magnitude_pred.std():.4f}, Range: [{y_magnitude_pred.min():.4f}, {y_magnitude_pred.max():.4f}]")

    # Correlation
    correlation = np.corrcoef(y_magnitude_test_events, y_magnitude_pred)[0, 1]
    print(f"  Correlation: {correlation:.4f}")

# Model 3: Traffic Light
print("\nüìä MODEL 3: 3-CLASS TRAFFIC LIGHT CLASSIFICATION")
print("-" * 80)
y_traffic_pred = model_traffic.predict(X_test).flatten()

print("\nClassification Report:")
print(classification_report(y_traffic_test, y_traffic_pred,
                            target_names=['üü¢ GREEN', 'üü° YELLOW', 'üî¥ RED'],
                            zero_division=0))

cm_traffic = confusion_matrix(y_traffic_test, y_traffic_pred)
print(f"\nConfusion Matrix:")
print(cm_traffic)

# ==============================================================================
# STEP 9: INTEGRATED PREDICTION SYSTEM
# ==============================================================================
print("\n" + "=" * 80)
print("STEP 9: INTEGRATED PREDICTION SYSTEM")
print("=" * 80)

# Create integrated predictions
predictions_df = pd.DataFrame({
    'event_probability': y_event_proba,
    'event_predicted': y_event_pred,
    'event_actual': y_event_test
})

# Add magnitude predictions for ALL samples (0 for non-events)
if model_magnitude is not None:
    magnitude_pred_all = np.zeros(len(X_test))
    # Only predict magnitude for predicted events
    if y_event_pred.sum() > 0:
        X_predicted_events = X_test.iloc[y_event_pred == 1].reset_index(drop=True)
        magnitude_pred_all[y_event_pred == 1] = model_magnitude.predict(X_predicted_events)

    predictions_df['magnitude_predicted'] = magnitude_pred_all
    predictions_df['magnitude_actual'] = y_magnitude_test

# Add traffic light
predictions_df['traffic_light_predicted'] = y_traffic_pred
predictions_df['traffic_light_actual'] = y_traffic_test

# Map traffic light to labels
traffic_labels = {0: 'üü¢ GREEN', 1: 'üü° YELLOW', 2: 'üî¥ RED'}
predictions_df['traffic_light_label_pred'] = predictions_df['traffic_light_predicted'].map(traffic_labels)
predictions_df['traffic_light_label_actual'] = predictions_df['traffic_light_actual'].map(traffic_labels)

# Save predictions
predictions_df.to_csv('seismic_predictions_v2.csv', index=False)
print("üíæ Saved: seismic_predictions_v2.csv")

# Display sample predictions
print("\nüìã SAMPLE PREDICTIONS (First 20 test samples):")
print("-" * 120)
display_cols = ['event_probability', 'event_predicted', 'traffic_light_label_pred']
if model_magnitude is not None:
    display_cols.insert(2, 'magnitude_predicted')
print(predictions_df[display_cols].head(20).to_string(index=False))

# Show some actual event predictions
print("\nüìã PREDICTED EVENT SAMPLES (First 10 predicted events):")
print("-" * 120)
event_predictions = predictions_df[predictions_df['event_predicted'] == 1]
if len(event_predictions) > 0:
    display_cols_events = ['event_probability', 'magnitude_predicted', 'traffic_light_label_pred', 'event_actual', 'traffic_light_label_actual']
    if model_magnitude is not None:
        print(event_predictions[display_cols_events].head(10).to_string(index=False))
    else:
        print(event_predictions[['event_probability', 'traffic_light_label_pred', 'event_actual', 'traffic_light_label_actual']].head(10).to_string(index=False))
else:
    print("  No events predicted in test set")

print(f"\n{'='*80}")
print("‚úÖ SEISMIC TRAFFIC LIGHT SYSTEM v2 TRAINING COMPLETE!")
print(f"{'='*80}")
print("\nüì¶ Generated Files:")
print("  1. seismic_event_occurrence_model_v2.cbm      - Event probability predictor")
print("  2. seismic_magnitude_model_v2.cbm             - Magnitude estimator (events only)")
print("  3. seismic_traffic_light_3class_model_v2.cbm  - 3-Class traffic light (no ORANGE)")
print("  4. train_medians_v2.pkl                       - Training statistics")
print("  5. optimal_event_threshold_v2.txt             - Optimal event threshold")
print("  6. seismic_predictions_v2.csv                 - Test predictions")
print(f"\n‚ú® Improvements:")
print("  ‚Ä¢ 3-class traffic light (GREEN, YELLOW, RED)")
print("  ‚Ä¢ COVID period GREEN downsampling (2021-2022)")
print("  ‚Ä¢ Fixed indexing for magnitude model")
print("  ‚Ä¢ No SettingWithCopyWarning")
print(f"\nüö¶ Traffic Light System v2 Ready for Operational Deployment!")
print(f"{'='*80}\n")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_recall_curve, ConfusionMatrixDisplay, average_precision_score
from sklearn.preprocessing import label_binarize
import os

# Create folder to save plots
os.makedirs("plots_v2", exist_ok=True)

# -------------------------
# 1Ô∏è‚É£ MODEL 1: Event Occurrence Confusion Matrix
# -------------------------
plt.figure(figsize=(6,5))
cm_event_display = ConfusionMatrixDisplay(cm_event, display_labels=['No Event', 'Event'])
cm_event_display.plot(cmap='Blues', values_format='d', ax=plt.gca())
plt.title("Model 1 - Event Occurrence Confusion Matrix")
plt.tight_layout()
plt.savefig("plots_v2/model1_event_confusion_matrix.png")
plt.close()

# -------------------------
# 2Ô∏è‚É£ MODEL 2: Magnitude Prediction Regression Heatmap
# -------------------------
if model_magnitude is not None and len(X_test_events) > 0:
    # Correlation heatmap
    plt.figure(figsize=(6,5))
    corr_matrix = pd.DataFrame({'Actual': y_magnitude_test_events, 'Predicted': y_magnitude_pred}).corr()
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title("Model 2 - Magnitude Prediction Correlation Heatmap")
    plt.tight_layout()
    plt.savefig("plots_v2/model2_magnitude_heatmap.png")
    plt.close()

    # Scatter plot
    plt.figure(figsize=(6,5))
    sns.scatterplot(x=y_magnitude_test_events, y=y_magnitude_pred, alpha=0.6)
    plt.plot([y_magnitude_test_events.min(), y_magnitude_test_events.max()],
             [y_magnitude_test_events.min(), y_magnitude_test_events.max()],
             'r--', lw=2)
    plt.xlabel("Actual Magnitude")
    plt.ylabel("Predicted Magnitude")
    plt.title("Model 2 - Actual vs Predicted Magnitude")
    plt.tight_layout()
    plt.savefig("plots_v2/model2_magnitude_scatter.png")
    plt.close()

# -------------------------
# 3Ô∏è‚É£ MODEL 3: 3-Class Traffic Light Precision-Recall Curve & Confusion Matrix
# -------------------------
y_true_bin = label_binarize(y_traffic_test, classes=[0,1,2])
y_score_bin = model_traffic.predict_proba(X_test)

plt.figure(figsize=(8,6))
for i, color in zip(range(3), ['green', 'orange', 'red']):
    precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_score_bin[:, i])
    ap_score = average_precision_score(y_true_bin[:, i], y_score_bin[:, i])
    plt.plot(recall, precision, color=color, lw=2, label=f"{traffic_labels[i]} (AP={ap_score:.2f})")

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Model 3 - 3-Class Traffic Light Precision-Recall Curve")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("plots_v2/model3_traffic_pr_curve.png")
plt.close()

# Confusion Matrix
plt.figure(figsize=(6,5))
cm_traffic_display = ConfusionMatrixDisplay(cm_traffic, display_labels=list(traffic_labels.values()))
cm_traffic_display.plot(cmap='Oranges', values_format='d', ax=plt.gca())
plt.title("Model 3 - Traffic Light Confusion Matrix")
plt.tight_layout()
plt.savefig("plots_v2/model3_traffic_confusion_matrix.png")
plt.close()

# -------------------------
# 4Ô∏è‚É£ FEATURE IMPORTANCE BAR Graphs
# -------------------------
def save_feature_importance(model, X, title, filename):
    importance = model.get_feature_importance()
    features = X.columns
    feat_imp_df = pd.DataFrame({'Feature': features, 'Importance': importance})
    feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False).head(20)

    plt.figure(figsize=(8,6))
    sns.barplot(x='Importance', y='Feature', data=feat_imp_df, palette='viridis')
    plt.title(title)
    plt.tight_layout()
    plt.savefig(filename)
    plt.close()

save_feature_importance(model_event, X_train, "Top 20 Feature Importance - Event Occurrence",
                        "plots_v2/feature_importance_event.png")
if model_magnitude is not None:
    save_feature_importance(model_magnitude, X_train_events, "Top 20 Feature Importance - Magnitude Prediction",
                            "plots_v2/feature_importance_magnitude.png")
save_feature_importance(model_traffic, X_train, "Top 20 Feature Importance - Traffic Light Classification",
                        "plots_v2/feature_importance_traffic.png")

print("‚úÖ All plots saved in folder: plots_v2")
