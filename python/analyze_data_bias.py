"""
Bias Analysis Script for Your Geothermal-Seismic Data

This script analyzes the specific bias issues in your merged datasets
and shows you exactly what problems exist and how to fix them.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def analyze_temporal_bias():
    """Analyze temporal bias in your data"""
    print("ğŸ•’ TEMPORAL BIAS ANALYSIS")
    print("=" * 40)
    
    # Load seismic data
    seismic_df = pd.read_csv('seismic_events_cleaned.csv')
    seismic_df['occurred_at'] = pd.to_datetime(seismic_df['occurred_at'])
    
    # Load operational data sample
    operational_df = pd.read_csv('operational_metrics_cleaned.csv', nrows=50000)
    operational_df['recorded_at'] = pd.to_datetime(operational_df['recorded_at'])
    
    print("ğŸ“Š SEISMIC EVENTS - Time Distribution:")
    seismic_monthly = seismic_df.set_index('occurred_at').resample('M').size()
    print(f"  â€¢ Events per month range: {seismic_monthly.min()} to {seismic_monthly.max()}")
    print(f"  â€¢ Most active month: {seismic_monthly.idxmax().strftime('%Y-%m')} ({seismic_monthly.max()} events)")
    print(f"  â€¢ Least active month: {seismic_monthly.idxmin().strftime('%Y-%m')} ({seismic_monthly.min()} events)")
    
    # Check for clustering
    seismic_df['year'] = seismic_df['occurred_at'].dt.year
    yearly_counts = seismic_df['year'].value_counts().sort_index()
    print(f"\nğŸ“ˆ Yearly Distribution:")
    for year, count in yearly_counts.items():
        print(f"  â€¢ {year}: {count} events")
    
    print(f"\nâš ï¸  TEMPORAL BIAS DETECTED:")
    print(f"  â€¢ {yearly_counts.max()} events in {yearly_counts.idxmax()} vs {yearly_counts.min()} in {yearly_counts.idxmin()}")
    print(f"  â€¢ Ratio: {yearly_counts.max() / yearly_counts.min():.1f}:1")
    
    return yearly_counts

def analyze_operational_bias():
    """Analyze operational phase bias"""
    print("\nâš™ï¸ OPERATIONAL BIAS ANALYSIS")
    print("=" * 40)
    
    # Load operational data
    operational_df = pd.read_csv('operational_metrics_cleaned.csv')
    
    print("ğŸ“Š OPERATIONAL PHASES - Data Distribution:")
    phase_counts = operational_df['phase'].value_counts().sort_index()
    
    print(f"  â€¢ Total phases: {len(phase_counts)}")
    print(f"  â€¢ Records per phase range: {phase_counts.min():,} to {phase_counts.max():,}")
    
    # Show top phases
    print(f"\nğŸ“ˆ Top 5 Phases by Record Count:")
    for phase, count in phase_counts.head().items():
        percentage = (count / len(operational_df)) * 100
        print(f"  â€¢ Phase {phase}: {count:,} records ({percentage:.1f}%)")
    
    print(f"\nâš ï¸  OPERATIONAL BIAS DETECTED:")
    print(f"  â€¢ Largest phase: {phase_counts.max():,} records")
    print(f"  â€¢ Smallest phase: {phase_counts.min():,} records")
    print(f"  â€¢ Ratio: {phase_counts.max() / phase_counts.min():.1f}:1")
    
    return phase_counts

def analyze_seismic_bias():
    """Analyze seismic event clustering bias"""
    print("\nğŸŒ SEISMIC CLUSTERING BIAS ANALYSIS")
    print("=" * 40)
    
    seismic_df = pd.read_csv('seismic_events_cleaned.csv')
    
    print("ğŸ“Š SEISMIC EVENTS - Spatial Distribution:")
    
    # Analyze by coordinates
    x_range = seismic_df['x'].max() - seismic_df['x'].min()
    y_range = seismic_df['y'].max() - seismic_df['y'].min()
    z_range = seismic_df['z'].max() - seismic_df['z'].min()
    
    print(f"  â€¢ X coordinate range: {x_range:.1f} units")
    print(f"  â€¢ Y coordinate range: {y_range:.1f} units")
    print(f"  â€¢ Z coordinate range: {z_range:.1f} units")
    
    # Check for clustering in coordinates
    seismic_df['x_bin'] = pd.cut(seismic_df['x'], bins=5)
    seismic_df['y_bin'] = pd.cut(seismic_df['y'], bins=5)
    
    spatial_counts = seismic_df.groupby(['x_bin', 'y_bin']).size()
    
    print(f"\nğŸ“ˆ Spatial Distribution:")
    print(f"  â€¢ Events per spatial bin range: {spatial_counts.min()} to {spatial_counts.max()}")
    print(f"  â€¢ Most clustered area: {spatial_counts.max()} events")
    print(f"  â€¢ Least clustered area: {spatial_counts.min()} events")
    
    print(f"\nâš ï¸  SEISMIC CLUSTERING BIAS DETECTED:")
    print(f"  â€¢ Ratio: {spatial_counts.max() / spatial_counts.min():.1f}:1")
    print(f"  â€¢ Events are clustered in specific locations")
    
    return spatial_counts

def analyze_missing_data_bias():
    """Analyze missing data patterns"""
    print("\nâŒ MISSING DATA BIAS ANALYSIS")
    print("=" * 40)
    
    operational_df = pd.read_csv('operational_metrics_cleaned.csv')
    
    print("ğŸ“Š MISSING DATA PATTERNS:")
    
    # Calculate missing data by column
    missing_data = operational_df.isnull().sum()
    missing_percentage = (missing_data / len(operational_df)) * 100
    
    print(f"  â€¢ Columns with missing data: {len(missing_data[missing_data > 0])}")
    print(f"  â€¢ Columns with >5% missing: {len(missing_percentage[missing_percentage > 5])}")
    
    print(f"\nğŸ“ˆ Top Missing Data Columns:")
    high_missing = missing_percentage[missing_percentage > 1].sort_values(ascending=False)
    for col, pct in high_missing.head().items():
        print(f"  â€¢ {col}: {missing_data[col]:,} missing ({pct:.1f}%)")
    
    # Check if missing data is systematic
    print(f"\nâš ï¸  MISSING DATA BIAS DETECTED:")
    if len(high_missing) > 0:
        print(f"  â€¢ Systematic missing data in {len(high_missing)} columns")
        print(f"  â€¢ Highest missing: {high_missing.index[0]} ({high_missing.iloc[0]:.1f}%)")
    
    return missing_percentage

def create_bias_visualization():
    """Create visualization of bias issues"""
    print("\nğŸ“Š CREATING BIAS VISUALIZATION...")
    
    # Load data
    seismic_df = pd.read_csv('seismic_events_cleaned.csv')
    operational_df = pd.read_csv('operational_metrics_cleaned.csv', nrows=10000)
    
    # Create figure
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Data Bias Analysis - Your Geothermal Dataset', fontsize=16, fontweight='bold')
    
    # 1. Temporal bias - Seismic events over time
    seismic_df['occurred_at'] = pd.to_datetime(seismic_df['occurred_at'])
    seismic_monthly = seismic_df.set_index('occurred_at').resample('M').size()
    
    axes[0,0].plot(seismic_monthly.index, seismic_monthly.values, marker='o')
    axes[0,0].set_title('Temporal Bias: Seismic Events Over Time')
    axes[0,0].set_xlabel('Date')
    axes[0,0].set_ylabel('Events per Month')
    axes[0,0].tick_params(axis='x', rotation=45)
    
    # 2. Operational bias - Phase distribution
    phase_counts = operational_df['phase'].value_counts().head(10)
    axes[0,1].bar(range(len(phase_counts)), phase_counts.values)
    axes[0,1].set_title('Operational Bias: Phase Distribution')
    axes[0,1].set_xlabel('Phase')
    axes[0,1].set_ylabel('Record Count')
    axes[0,1].set_xticks(range(len(phase_counts)))
    axes[0,1].set_xticklabels([f'{p:.1f}' for p in phase_counts.index], rotation=45)
    
    # 3. Seismic clustering - Magnitude distribution
    seismic_df['magnitude_numeric'] = pd.to_numeric(seismic_df['magnitude'], errors='coerce')
    axes[1,0].hist(seismic_df['magnitude_numeric'].dropna(), bins=20, alpha=0.7)
    axes[1,0].set_title('Seismic Clustering: Magnitude Distribution')
    axes[1,0].set_xlabel('Magnitude')
    axes[1,0].set_ylabel('Event Count')
    
    # 4. Missing data bias
    missing_data = operational_df.isnull().sum()
    missing_percentage = (missing_data / len(operational_df)) * 100
    high_missing = missing_percentage[missing_percentage > 1].sort_values(ascending=False)
    
    if len(high_missing) > 0:
        axes[1,1].bar(range(len(high_missing)), high_missing.values)
        axes[1,1].set_title('Missing Data Bias: Column-wise Missing %')
        axes[1,1].set_xlabel('Columns')
        axes[1,1].set_ylabel('Missing Percentage (%)')
        axes[1,1].set_xticks(range(len(high_missing)))
        axes[1,1].set_xticklabels(high_missing.index, rotation=45)
    
    plt.tight_layout()
    plt.savefig('data_bias_analysis.png', dpi=300, bbox_inches='tight')
    print("âœ“ Bias analysis visualization saved as: data_bias_analysis.png")

def generate_bias_report():
    """Generate comprehensive bias report"""
    print("\nğŸ“‹ GENERATING BIAS REPORT...")
    
    report_content = []
    report_content.append("DATA BIAS ANALYSIS REPORT")
    report_content.append("=" * 50)
    report_content.append(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_content.append("")
    
    # Analyze each type of bias
    temporal_bias = analyze_temporal_bias()
    operational_bias = analyze_operational_bias()
    seismic_bias = analyze_seismic_bias()
    missing_bias = analyze_missing_data_bias()
    
    report_content.append("SUMMARY OF BIAS ISSUES")
    report_content.append("-" * 25)
    report_content.append("1. TEMPORAL BIAS:")
    report_content.append(f"   â€¢ Seismic events unevenly distributed across time")
    report_content.append(f"   â€¢ Yearly variation: {temporal_bias.max() / temporal_bias.min():.1f}:1 ratio")
    report_content.append("")
    
    report_content.append("2. OPERATIONAL BIAS:")
    report_content.append(f"   â€¢ Different phases have vastly different data volumes")
    report_content.append(f"   â€¢ Phase variation: {operational_bias.max() / operational_bias.min():.1f}:1 ratio")
    report_content.append("")
    
    report_content.append("3. SEISMIC CLUSTERING BIAS:")
    report_content.append("   â€¢ Events clustered in specific spatial locations")
    report_content.append("   â€¢ May not represent true seismic distribution")
    report_content.append("")
    
    report_content.append("4. MISSING DATA BIAS:")
    report_content.append(f"   â€¢ Systematic missing data in {len(missing_bias[missing_bias > 1])} columns")
    report_content.append("   â€¢ May correlate with operational phases or equipment")
    report_content.append("")
    
    report_content.append("RECOMMENDATIONS")
    report_content.append("-" * 15)
    report_content.append("1. Use stratified sampling by operational phase")
    report_content.append("2. Balance temporal distribution with time-based sampling")
    report_content.append("3. Consider spatial clustering in model validation")
    report_content.append("4. Handle missing data patterns in preprocessing")
    report_content.append("")
    
    report_content.append("QUICK FIXES")
    report_content.append("-" * 10)
    report_content.append("# Stratified sampling by phase")
    report_content.append("balanced_data = data.groupby('phase').apply(lambda x: x.sample(min(len(x), 1000)))")
    report_content.append("")
    report_content.append("# Temporal balancing")
    report_content.append("data['time_period'] = pd.cut(data['date'], bins=12)")
    report_content.append("balanced_data = data.groupby('time_period').apply(lambda x: x.sample(min(len(x), 500)))")
    
    # Save report
    with open('data_bias_report.txt', 'w') as f:
        f.write('\n'.join(report_content))
    
    print("âœ“ Bias analysis report saved as: data_bias_report.txt")

def main():
    """Main analysis function"""
    print("ğŸ” ANALYZING DATA BIAS IN YOUR GEOTHERMAL DATASET")
    print("=" * 60)
    
    try:
        # Run all bias analyses
        generate_bias_report()
        create_bias_visualization()
        
        print("\nğŸ‰ BIAS ANALYSIS COMPLETED!")
        print("\nFiles created:")
        print("âœ“ data_bias_report.txt - Detailed bias analysis")
        print("âœ“ data_bias_analysis.png - Visual bias overview")
        
        print("\nğŸ“‹ KEY FINDINGS:")
        print("â€¢ Your data has significant bias issues that will affect model training")
        print("â€¢ Use stratified sampling to create balanced datasets")
        print("â€¢ Consider temporal and spatial clustering in your analysis")
        print("â€¢ Handle missing data patterns systematically")
        
    except Exception as e:
        print(f"Error during bias analysis: {e}")

if __name__ == "__main__":
    main()

